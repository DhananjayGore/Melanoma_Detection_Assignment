## Problem Statement

The goal is to develop a CNN-based model capable of accurately detecting melanoma, a potentially deadly form of skin cancer that is responsible for 75% of skin cancer-related deaths. Early detection is crucial, and a solution that can analyze images and alert dermatologists to the presence of melanoma could significantly reduce the manual effort required in diagnosing this condition.

from google.colab import drive
drive.mount('/content/drive')
# Unzip the dataset
!unzip "/content/drive/MyDrive/CNN_assignment.zip" -d "/content/drive/MyDrive/"
**Importing all the important libraries**
import pathlib
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import PIL
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
# Set the paths to the Train and Test directories
# Using pathlib.Path to create Path objects for the directories
data_dir_train = pathlib.Path("/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train")
data_dir_test = pathlib.Path("/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Test")

image_count_train = len(list(data_dir_train.glob('*/*.jpg')))
print(image_count_train)
image_count_test = len(list(data_dir_test.glob('*/*.jpg')))
print(image_count_test)
# Verifying the contents
!ls "/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train"
**Create a dataset**
# Define some parameters for the loader
batch_size = 32
img_height = 180
img_width = 180
# Create the train dataset (80% of the images for training)
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    validation_split=0.2,           # Using 20% of the data for validation
    subset="training",
    seed=123,                       # Ensuring reproducibility
    image_size=(img_height, img_width),
    batch_size=batch_size
)

# Create the validation dataset (20% of the images for validation)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    validation_split=0.2,           # Using 20% of the data for validation
    subset="validation",
    seed=123,                       # Ensuring reproducibility
    image_size=(img_height, img_width),
    batch_size=batch_size
)

# List out all the classes of skin cancer
class_names = train_ds.class_names
print("Class names: ", class_names)
**Visualize the data**

import matplotlib.pyplot as plt

# Creating a function to visualize one instance of each class
def visualize_classes(dataset, class_names):
    plt.figure(figsize=(12, 12))
    class_seen = [False] * len(class_names)  # Track which classes have been visualized
    images_shown = 0

    # Loop through the dataset to get batches of images and labels
    for image_batch, label_batch in dataset:
        for i in range(len(image_batch)):
            label = label_batch[i].numpy()
            if not class_seen[label]:
                plt.subplot(3, 3, images_shown + 1)
                plt.imshow(image_batch[i].numpy().astype("uint8"))
                plt.title(class_names[label])
                plt.axis("off")
                class_seen[label] = True
                images_shown += 1
            if images_shown == len(class_names):  # Stop when all classes are visualized
                break
        if images_shown == len(class_names):
            break

    plt.show()

# Calling the function to visualize from the training or validation dataset
visualize_classes(train_ds, class_names)

The `image_batch` is a tensor with the shape `(32, 180, 180, 3)`, representing a batch of 32 images, each of size 180x180 pixels with 3 color channels (RGB). The `label_batch` is a tensor of shape `(32,)`, containing the corresponding labels for the 32 images in the batch.

- **`Dataset.cache()`**: This method stores the images in memory after they are initially loaded from the disk during the first epoch. This improves efficiency in subsequent epochs by preventing the need to reload the data from disk.

- **`Dataset.prefetch()`**: This method allows for overlapping data preprocessing and model execution during training, which ensures that the next batch of data is prepared while the current batch is being processed, reducing idle time and improving overall training performance.

AUTOTUNE = tf.data.experimental.AUTOTUNE

# Optimize the training dataset
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)

# Optimize the validation dataset
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

**Create the model**
# Define the CNN model with normalization
model = Sequential([
    # Rescale pixel values from [0, 255] to [0, 1]
    layers.Rescaling(1./255, input_shape=(180, 180, 3)),

    # First convolutional block
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Second convolutional block
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Third convolutional block
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Flatten the feature maps
    layers.Flatten(),

    # Fully connected layer
    layers.Dense(128, activation='relu'),

    # Output layer (for 9 classes)
    layers.Dense(9, activation='softmax')
])

# Print the model summary
model.summary()

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20
)
**Visualizing the results**

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc))  # Ensure the epochs range matches the length of the training

plt.figure(figsize=(12, 6))

# Plot Training and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plot Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

# Display the plots
plt.show()
## Findings from the First Model Execution

- **Accuracy**: 
  The model started with ~21% accuracy in the first epoch and improved to 88% by epoch 20 on the training set. However, validation accuracy peaked at 53-56%, indicating poor generalization.

- **Loss**: 
  Training loss decreased as expected, but validation loss started increasing after epoch 6, suggesting overfitting. By epoch 20, validation loss was 2.3244, while training loss remained much lower.

- **Overfitting**: 
  There is clear overfitting—good training performance but poor validation results, with validation loss rising and accuracy stagnating after epoch 6-7.

- **Imbalance in Performance**: 
  The large gap between training accuracy (88%) and validation accuracy (53%) highlights that the model is overfitting to the training data.

In this section, **data augmentation** is used on the training dataset to enhance generalization and reduce overfitting. The augmentation techniques include random flipping, rotation, zooming, and contrast adjustment. These transformations are applied dynamically during training using a `tf.keras.Sequential` model.

# Add data augmentation to the training dataset
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2)
])

# Apply data augmentation to the training dataset
augmented_train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y)
)

# Prefetch and cache the augmented dataset
AUTOTUNE = tf.data.experimental.AUTOTUNE
augmented_train_ds = augmented_train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)



In this section, a function is used to **visualize the augmented images**. It randomly applies the same data augmentation techniques (flip, rotation, zoom, contrast) on a sample of images from the training dataset and displays them.
import matplotlib.pyplot as plt

# Function to visualize augmented images
def visualize_augmented_images(dataset):
    plt.figure(figsize=(10, 10))
    for images, _ in dataset.take(1):
        for i in range(9):
            augmented_image = data_augmentation(images[i: i+1])
            plt.subplot(3, 3, i + 1)
            plt.imshow(augmented_image[0].numpy().astype("uint8"))
            plt.axis("off")
    plt.show()

# Visualize augmented images
visualize_augmented_images(train_ds)

# Train the model again with the augmented data
history_augmented = model.fit(
    augmented_train_ds,
    validation_data=val_ds,
    epochs=20
)
## Findings from the Second Model Execution

- **Training Accuracy vs. Validation Accuracy**: 
  Training accuracy is high (e.g., 98% by epoch 19), but validation accuracy is stagnating or declining, indicating poor generalization.

- **Training Loss vs. Validation Loss**: 
  Training loss is decreasing, while validation loss is increasing (e.g., 5.9886 by epoch 20), a clear sign of overfitting as the model performs well on training data but struggles with unseen data.

In this section, a Convolutional Neural Network (CNN) model is defined with added **Dropout layers** to **reduce overfitting**. Dropout layers randomly deactivate a fraction of the neurons during training, which helps the model generalize better to new data by preventing reliance on specific neurons.
# Updated CNN model with Dropout layers
model = Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),

    # First convolutional block
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.3),  # Add dropout to regularize the model

    # Second convolutional block
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.3),

    # Third convolutional block
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Flatten the feature maps
    layers.Flatten(),

    # Fully connected layer with Dropout
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),  # Dropout to prevent overfitting

    # Output layer (9 classes)
    layers.Dense(len(class_names), activation='softmax')
])

# Print the model summary
model.summary()

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20
)
**Visualizing the results**
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(20)  # If you trained for 20 epochs, adjust as needed

plt.figure(figsize=(10, 10))

# Plot Training and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plot Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

## Findings from the Third Model Execution

- **Training Loss & Accuracy**: 
  Training accuracy started at 19.09% and improved to 58.03% by epoch 20. Training loss decreased from 2.7375 to 1.1553, indicating learning over time.

- **Validation Loss & Accuracy**: 
  Validation accuracy began at 19.24% and ended at 49.89%. Validation loss fluctuated from 2.1009 to 1.5175, with no consistent decrease.

- **Evidence of Underfitting or Overfitting**:

  - *Underfitting*: 
    Early on, both training and validation accuracy were low. Training accuracy improved, but validation accuracy lagged, indicating the model may have underfitted and struggled to generalize.

  - *Overfitting*: 
    Later epochs showed a widening gap between training and validation accuracy, with validation loss not improving significantly, suggesting potential overfitting as the model started memorizing the training data.

## Improvement Comparison between Models

### Model 1 (Augmented Data)

- **Training Accuracy**: 98.21%
- **Validation Accuracy**: 49.22%
- **Training Loss**: 0.08 (indicating strong fit to training data)
- **Validation Loss**: 5.98 (indicating overfitting)

### Model 2 (Dropout Layers)

- **Training Accuracy**: 58.03%
- **Validation Accuracy**: 49.89%
- **Training Loss**: 1.15 (learning, but slower than Model 1)
- **Validation Loss**: 1.51 (more stable than Model 1)

### Improvement Comparison

- **Overfitting**: Model 1 showed severe overfitting with a large gap between training and validation accuracy. Model 2, with dropout layers, has reduced overfitting, aligning training and validation accuracies better.
  
- **Validation Accuracy**: Model 2 maintains more consistent validation accuracy (49-55%) compared to the significant drop seen in Model 1.

- **Training Accuracy**: Model 1's high training accuracy was due to overfitting. Model 2's training accuracy is more balanced, though still room for improvement.

### Conclusion

Yes, Model 2 shows improvement over Model 1 by reducing overfitting and stabilizing performance. However, validation accuracy could still improve, and further tuning or architectural adjustments may be necessary.

**Find the distribution of classes in the training dataset.**

**Context**: Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data.
In this section, the original training dataset is being loaded, and the class distribution of images is analyzed before any data augmentation.
import pandas as pd
import os
import glob

# Path to training dataset
path_to_training_dataset = pathlib.Path("/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train")

# Extract all the original image paths (excluding augmented images in the 'output' folder)
path_list_original = [x for x in glob.glob(os.path.join(path_to_training_dataset, '*', '*.jpg'))]

# Extract class labels for each image from the directory structure
lesion_list_original = [os.path.basename(os.path.dirname(y)) for y in path_list_original]

# Create a DataFrame for the original images
original_df = pd.DataFrame({
    'Path': path_list_original,
    'Label': lesion_list_original
})

# Check the class distribution in the original dataset
class_distribution = original_df['Label'].value_counts()
print("Class distribution before augmentation:")
print(class_distribution)

# Visualize the class distribution using a bar plot
class_distribution.plot(kind='bar', title='Class Distribution in Training Dataset')
plt.show()

## Findings from the Class Distribution

- **Class with the Least Number of Samples**:  
  **Seborrheic Keratosis** has the fewest samples, with only 77 images.

- **Classes Dominating the Data**:  
  **Pigmented Benign Keratosis** (462 samples) and **Melanoma** (438 samples) have the highest sample counts, dominating the dataset.

**Rectify Class Imbalance Using Augmentor**
Once you've identified class imbalance, we can use the Augmentor library to generate more samples for the underrepresented classes.

**First, install Augmentor:**
!pip install Augmentor

This section demonstrates how data augmentation is used to rectify class imbalance in the dataset:

**Augmentor Pipeline**: For each class in the dataset, an Augmentor pipeline is applied to create additional training samples using techniques such as rotation, horizontal flipping, and random zoom. These augmentations help diversify the dataset and mitigate the class imbalance by artificially expanding the number of samples per class.

**Sample Generation**: The code generates 500 new samples per class through these transformations. The number of samples can be adjusted depending on the degree of imbalance and the desired balance level.

**Re-check Class Distribution**: After the augmentation process, the class distribution is re-calculated and visualized to ensure that the classes are more balanced. This post-augmentation visualization provides a clear view of the effect of the augmentation process on the dataset.
import Augmentor

# Rectify class imbalance by augmenting each class to ensure each class has a sufficient number of samples
for class_name in original_df['Label'].unique():
    class_path = os.path.join(path_to_training_dataset, class_name)  # Path to each class directory
    p = Augmentor.Pipeline(class_path)  # Create Augmentor pipeline for each class

    # Apply transformations
    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)  # Rotation
    p.flip_left_right(probability=0.5)  # Flip horizontally
    p.zoom_random(probability=0.5, percentage_area=0.8)  # Random zoom

    # Generate 500 new samples per class (you can adjust this number as needed)
    p.sample(500)

# After applying augmentation, re-check the class distribution
augmented_image_paths = [x for x in glob.glob(os.path.join(path_to_training_dataset, '*', 'output', '*.jpg'))]
augmented_labels = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in augmented_image_paths]

# Create a new dataframe with augmented data
df_augmented = pd.DataFrame({'Path': augmented_image_paths, 'Label': augmented_labels})

# Combine the original and augmented data
combined_df = pd.concat([original_df, df_augmented], ignore_index=True)

# Check the class distribution again after augmentation
new_class_distribution = combined_df['Label'].value_counts()
print("Class distribution after augmentation:")
print(new_class_distribution)

# Visualize the updated class distribution
new_class_distribution.plot(kind='bar', title='Class Distribution After Augmentation')
plt.show()

## Findings After Augmentation

- **Class with the Least Number of Samples After Augmentation**:  
  **Seborrheic Keratosis** still has the fewest samples, now with 577 images, although its size has increased.

- **Are the Classes More Balanced Now?**  
  Yes, the classes are more balanced post-augmentation. While **Pigmented Benign Keratosis** still has the most samples (962), the gap between it and the other classes is now much smaller, improving overall dataset balance. This should help the model generalize better across all classes.

**Train the model on the data created using Augmentor**
batch_size = 32
img_height = 180
img_width = 180
data_dir_train = pathlib.Path("/content/drive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train")

**Create a training and validation dataset**
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    seed=123,
    validation_split=0.2,  # 20% of data used for validation
    subset="training",     # This ensures only the training data is loaded
    image_size=(img_height, img_width),
    batch_size=batch_size
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    seed=123,
    validation_split=0.2,  # 20% of data used for validation
    subset="validation",   # This ensures only the validation data is loaded
    image_size=(img_height, img_width),
    batch_size=batch_size
)

**Model Creation**
# Define the CNN model with normalization and dropout layers
model = tf.keras.Sequential([
    # Rescaling to normalize pixel values between 0 and 1
    tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),

    # First convolutional block
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting

    # Second convolutional block
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting

    # Third convolutional block
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting

    # Fourth convolutional block (New addition for extra complexity)
    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    # Flatten the feature maps
    tf.keras.layers.Flatten(),

    # Fully connected layer with dropout
    tf.keras.layers.Dense(256, activation='relu'),  # Increased size for complexity
    tf.keras.layers.Dropout(0.5),  # Dropout to prevent overfitting in dense layers

    # Output layer (assuming 9 classes)
    tf.keras.layers.Dense(9, activation='softmax')  # Update number of classes if needed
])

# Print the model summary
model.summary()

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

epochs = 50  # increased this to 50 as mentioned in the instructions

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

**Visualize the model results**
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

## Findings and Analysis

- **Training Accuracy**:  
  Training accuracy steadily improved from 14% to 85% over 50 epochs, indicating successful learning of the training data patterns.

- **Validation Accuracy**:  
  Validation accuracy improved from 16% to 62%, showing better generalization but still indicating **mild overfitting**, as the training accuracy is much higher.

- **Training Loss**:  
  Training loss decreased significantly from 2.27 to 0.36, confirming effective learning on the training data.

- **Validation Loss**:  
  Validation loss fluctuated, starting at 2.15 and ending at 1.69, indicating the model struggled more with the validation data.

### Overfitting/Underfitting:

- **Overfitting**:  
  The gap between training accuracy (85%) and validation accuracy (62%) suggests mild overfitting. The training loss consistently decreased while the validation loss fluctuated, indicating the model fits the training data well but struggles to generalize.

  **Suggestion**: To mitigate overfitting, try using stronger regularization techniques like increased dropout, L2 regularization, or more extensive data augmentation.

- **Underfitting**:  
  There’s no strong evidence of underfitting, as the model performed well on the training data.

### Impact of Class Rebalance:

- **Class Balance Improvement**:  
  After rebalancing the dataset using augmentation, training accuracy improved, and validation accuracy showed a better trend compared to previous runs without rebalancing.

  **Result**: Rebalancing helped prevent the model from focusing too heavily on more frequent classes, leading to more balanced learning.

### Further Steps:
While class rebalancing has improved performance, additional improvements can be made by fine-tuning the data augmentation strategy or adjusting the model architecture for more complex patterns.

**This code snippet demonstrates to *visualize and predict a single test image* using the trained CNN model**
from glob import glob
from tensorflow.keras.preprocessing.image import load_img
Test_image_path = os.path.join(data_dir_test, class_names[1], '*')
Test_image = glob(Test_image_path)
Test_image = load_img(Test_image[-1],target_size=(180,180,3))
plt.imshow(Test_image)
plt.grid(False)

img = np.expand_dims(Test_image,axis=0)
pred = model.predict(img)
pred = np.argmax(pred)
pred_class = class_names[pred]
print("Actual Class "+ class_names[1] +'\n'+ "Predictive Class "+pred_class )
